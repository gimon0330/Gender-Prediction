{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec291af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gimon\\AppData\\Local\\Temp\\ipykernel_5172\\2844206087.py:9: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-white\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-white\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5612f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_male = \"dataset/Training/male\"\n",
    "img_train_female = \"dataset/Training/female\"\n",
    "img_valid_male = \"dataset/Validation/male\"\n",
    "img_valid_female = \"dataset/Validation/female\"\n",
    "size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a1d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = keras.utils.load_img(img_path, target_size=(size,size))\n",
    "    tensor = keras.utils.img_to_array(img)\n",
    "    tensor /= 255.0 \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58a82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"male\", \"female\"]\n",
    "nb_class = len(categories)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# variables to test the face with a mask\n",
    "test_on_male_x = []\n",
    "test_on_male_y = []\n",
    "\n",
    "# variables to test the face with a mask\n",
    "test_female_x = []\n",
    "test_female_y = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eee2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=0\n",
    "# preprocess the images, each of which is a face with mask\n",
    "for i in os.listdir(img_train_male):\n",
    "    img_path = os.path.join(img_train_male, i)\n",
    "    img_tensor = preprocess_image(img_path)\n",
    "    x.append(img_tensor)\n",
    "    y.append(0)\n",
    " #   if a < 10:\n",
    " #       print (\"len\\[x\\]:  %d\" % len(x) )\n",
    " #   a=a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17876053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images, each of which is a face without mask\n",
    "for i in os.listdir(img_train_female):\n",
    "    img_path = os.path.join(img_train_female, i)\n",
    "    img_tensor = preprocess_image(img_path)\n",
    "    x.append(img_tensor)\n",
    "    y.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458cf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1)\n",
    "Y_train = keras.utils.to_categorical(Y_train, 2)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d895f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52f55f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64, 64, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 31, 31, 96)        76896     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 31, 31, 96)        384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 15, 15, 96)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 15, 15, 256)       221440    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 15, 15, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 15, 15, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 15, 15, 96)        221280    \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 15, 15, 96)        384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 7, 7, 96)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4704)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              4817920   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6062914 (23.13 MB)\n",
      "Trainable params: 6061442 (23.12 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# Conv2D\n",
    "# the # of convolution filters \n",
    "# the number of rows and columns in terms of a convolution kernel\n",
    "# padding\n",
    "#...\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), padding = \"same\", activation = \"relu\", input_shape=(size, size, 3)))\n",
    "################## start - add custum layers\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = (3, 3), strides = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(96, kernel_size = (5, 5), strides = (1, 1), padding = \"same\", kernel_regularizer=regularizers.l2(l2 = 1e-4), activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=regularizers.l2(l2 = 1e-4), activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=regularizers.l2(l2 = 1e-4), activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=regularizers.l2(l2 = 1e-4), activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "################## end - add custum layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b40bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e8bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "import cv2, sklearn\n",
    "\n",
    "def zero_one_scaler(image):\n",
    "    return image/255.0\n",
    "\n",
    "def get_preprocessed_ohe(images, labels, pre_func=None):\n",
    "    # preprocessing 함수가 입력되면 이를 이용하여 image array를 scaling 적용.\n",
    "    if pre_func is not None:\n",
    "        images = pre_func(images)\n",
    "    # OHE 적용    \n",
    "    oh_labels = to_categorical(labels)\n",
    "    return images, oh_labels\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, images_array, labels, batch_size = BATCH_SIZE, augmentor = None, shuffle = False, pre_func = None):\n",
    "        self.images_array = images_array\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = zero_one_scaler\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        images_fetch = self.images_array[index * self.batch_size:(index+1) * self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            label_batch = self.labels[index * self.batch_size:(index+1) * self.batch_size]\n",
    "        image_batch = np.zeros((images_fetch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3), dtype='float32')\n",
    "        \n",
    "        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n",
    "        for image_index in range(images_fetch.shape[0]):\n",
    "            #image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n",
    "            # 원본 image를 IMAGE_SIZE x IMAGE_SIZE 크기로 변환\n",
    "            image = cv2.resize(images_fetch[image_index], (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            # 만약 augmentor가 주어졌다면 이를 적용. \n",
    "            if self.augmentor is not None:\n",
    "                image = self.augmentor(image=image)['image']\n",
    "                \n",
    "            # 만약 scaling 함수가 입력되었다면 이를 적용하여 scaling 수행. \n",
    "            if self.pre_func is not None:\n",
    "                image = self.pre_func(image)\n",
    "            \n",
    "            # image_batch에 순차적으로 변환된 image를 담음.               \n",
    "            image_batch[image_index] = image\n",
    "        \n",
    "        return image_batch, label_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: self.images_array, self.labels = sklearn.utils.shuffle(self.images_array, self.labels)\n",
    "    \n",
    "tr_ds = Dataset(X_train, Y_train, shuffle = True)\n",
    "val_ds = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87af9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7b3a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  16/1190 [..............................] - ETA: 17:17 - loss: 2.7734 - accuracy: 0.5762"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5, mode = 'min', verbose = 1)\n",
    "ely_cb = EarlyStopping(monitor = 'val_loss', patience = 4, mode = 'min', verbose = 1)\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 30, validation_split = 0.1, callbacks = [rlr_cb, ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a deep learning model\n",
    "prediction = model.predict(X_test)\n",
    "loss, acc = model.evaluate(X_test, Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # draw images with accuracies and labels\n",
    "topCnt = 8 *10\n",
    "if len(X_test) < topCnt:\n",
    "    topCnt = len(X_test)\n",
    "    \n",
    "plt.figure(figsize=(15,15))\n",
    "for idx in range(topCnt):\n",
    "    plt.subplot(8, 10, idx+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[idx], cmap=plt.cm.binary)\n",
    "    \n",
    "    if prediction[idx][0] > prediction[idx][1]:\n",
    "        label = \"Male \" + str(int(prediction[idx][0] * 100)) + \" %\"\n",
    "    else:\n",
    "        label = \"Female \" + str(int(prediction[idx][1] * 100)) + \" %\"\n",
    "    plt.xlabel(label)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condanote",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
